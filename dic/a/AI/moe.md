# MoE(Mixture of Experts)

Transformer ëª¨ë¸ì—ì„œ MoE (Mixture of Experts)ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì¸ Transformer ëª¨ë¸ë³´ë‹¤ ë³µì¡í•˜ì§€ë§Œ, ìµœê·¼ í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì£¼ìš” ê³ ë ¤ ì‚¬í•­ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

---

## 1. **MoEì˜ ê¸°ë³¸ ê°œë…**
MoEëŠ” ì—¬ëŸ¬ ê°œì˜ ì „ë¬¸ê°€ (Experts) ë„¤íŠ¸ì›Œí¬ ì¤‘ì—ì„œ ì…ë ¥ì— ë”°ë¼ ì¼ë¶€ë§Œ í™œì„±í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ê³„ì‚°ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ëª¨ë¸ ìš©ëŸ‰ì„ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. MoEëŠ” ì£¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ í•µì‹¬ ìš”ì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

- **Experts**: ë…ë¦½ì ì¸ FFN (Feed-Forward Network) ë ˆì´ì–´ë“¤ë¡œ êµ¬ì„±ë¨.
- **Router**: íŠ¹ì • ì…ë ¥ì´ ì–´ëŠ Expertsë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•¨.

---

## 2. **Transformerì— MoE ì ìš© ì‹œ ê³ ë ¤í•  ì **
MoEë¥¼ Transformer ëª¨ë¸ì— ì¶”ê°€í•  ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê³ ë ¤ ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤.

### **(1) MoEë¥¼ ì ìš©í•  ìœ„ì¹˜**
- Transformerì˜ **FFN (Feed-Forward Network) ë ˆì´ì–´**ë¥¼ MoEë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ì‹ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
- ì¦‰, `Transformer Encoder` ë˜ëŠ” `Decoder`ì˜ ê° ë¸”ë¡ ë‚´ì—ì„œ ê¸°ì¡´ FFN ëŒ€ì‹  ì—¬ëŸ¬ Experts ì¤‘ ì¼ë¶€ë¥¼ ì„ íƒí•˜ì—¬ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.

### **(2) Routing Mechanism**
- MoEì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ê°€ ëª¨ë“  Expertsë¥¼ ê±°ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì¼ë¶€ ì „ë¬¸ê°€ë“¤ë§Œ í™œì„±í™”ë©ë‹ˆë‹¤.
- ë³´í¸ì ìœ¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ ë°©ì‹ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
  - **Top-K Routing**: ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ Kê°œì˜ Expertsë§Œ ì„ íƒ.
  - **Soft Routing**: ì—¬ëŸ¬ Expertsì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ì„ í˜• ê²°í•©.

- Googleì˜ GShard ë° Switch Transformer ë…¼ë¬¸ì—ì„œëŠ” `Top-1` ë˜ëŠ” `Top-2` Expertsë§Œ í™œì„±í™”í•˜ëŠ” ë°©ì‹ì´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

### **(3) Load Balancing**
- íŠ¹ì • Expertsë§Œ ê³¼ë„í•˜ê²Œ ì„ íƒë˜ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë¡œë“œ ë°¸ëŸ°ì‹± ì†ì‹¤(Loss Term)ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
- ëŒ€í‘œì ì¸ ê¸°ë²•: **Load-Balancing Loss** (L1 Regularization)

### **(4) Efficient Computation**
- GPUì—ì„œ í™œì„±í™”ëœ Expertsë§Œ ì—°ì‚°í•  ìˆ˜ ìˆë„ë¡ **Sparse Computation**ì´ í•„ìš”í•©ë‹ˆë‹¤.
- PyTorch ë° TensorFlowì—ì„œ ì œê³µí•˜ëŠ” **Fused Kernel**ì„ í™œìš©í•˜ë©´ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 3. **MoE Transformer êµ¬í˜„ ë°©ë²•**
MoEë¥¼ Transformerì— ì ìš©í•˜ëŠ” ë°©ë²•ì—ëŠ” ëª‡ ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤.

### **(1) ì§ì ‘ êµ¬í˜„**
- PyTorchì—ì„œ ì§ì ‘ Expertsë¥¼ ì •ì˜í•˜ê³ , Routing Mechanismì„ ì ìš©í•˜ëŠ” ë°©ì‹.
- Custom Moduleì„ í™œìš©í•˜ì—¬ FFNì„ MoEë¡œ ë³€ê²½.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MoELayer(nn.Module):
    def __init__(self, num_experts, input_dim, hidden_dim, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.experts = nn.ModuleList([nn.Linear(input_dim, hidden_dim) for _ in range(num_experts)])
        self.gate = nn.Linear(input_dim, num_experts)  # Routing network

    def forward(self, x):
        gate_logits = self.gate(x)  # (batch, num_experts)
        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1)  # (batch, top_k)

        outputs = torch.zeros_like(x)
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]
            expert_out = self.experts[expert_idx](x)  # ì„ íƒëœ Expertsë§Œ ì‚¬ìš©
            outputs += expert_out * F.softmax(top_k_values, dim=-1)[:, i].unsqueeze(-1)

        return outputs
```

### **(2) Hugging Face `transformers` í™œìš©**
- `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” **Switch Transformer** ë“±ê³¼ ê°™ì€ MoE ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.
- `T5-MoE`, `GPT-MoE` ë“±ì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš© ê°€ëŠ¥.

```python
from transformers import T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("google/switch-base-8")  # MoE T5 ëª¨ë¸
```

### **(3) DeepSpeed MoE í™œìš©**
- Microsoftì˜ **DeepSpeed**ëŠ” MoEë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ êµ¬í˜„ì„ ì œê³µí•˜ë©°, ë§¤ìš° í° ëª¨ë¸ì—ì„œë„ í™œìš© ê°€ëŠ¥.

```python
from deepspeed.moe.layer import MoE

moe_layer = MoE(hidden_size=512, expert=4, ep_size=1, use_residual=False)
output = moe_layer(input_tensor)
```

---

## 4. **MoE Transformerì˜ ì¥ì ê³¼ ë‹¨ì **
| ì¥ì  | ë‹¨ì  |
|------|------|
| ëª¨ë¸ í¬ê¸°ë¥¼ í¬ê²Œ í•˜ë©´ì„œë„ ê³„ì‚°ëŸ‰ì„ ì œí•œí•  ìˆ˜ ìˆìŒ (Sparse Activation) | Routing ë¹„ìš©ì´ ì¦ê°€í•˜ì—¬ Latencyê°€ ì¦ê°€í•  ê°€ëŠ¥ì„± |
| ê³„ì‚°ëŸ‰ì„ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ | ì¼ë¶€ Expertsì— ë¡œë“œê°€ ì§‘ì¤‘ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ (Load Imbalance) |
| í•™ìŠµ ì¤‘ íŠ¹ì • Expertsê°€ íŠ¹ì • ë°ì´í„°ì— íŠ¹í™”ë˜ë„ë¡ ìœ ë„ ê°€ëŠ¥ | ê¸°ì¡´ Transformerë³´ë‹¤ ë³µì¡í•œ êµ¬í˜„ì´ í•„ìš” |

---

## 5. **ê²°ë¡ **
MoEë¥¼ Transformerì— ì ìš©í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì¸ Transformerë³´ë‹¤ ë³µì¡í•˜ì§€ë§Œ, **DeepSpeed, Hugging Face `transformers`, ë˜ëŠ” ìì²´ êµ¬í˜„**ì„ í™œìš©í•˜ë©´ ì ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

- ê°„ë‹¨í•œ MoE ì ìš©: `Hugging Face`ì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©.
- ì»¤ìŠ¤í…€ MoE ì ìš©: `PyTorch`ì—ì„œ ì§ì ‘ Routing ë° Expertsë¥¼ êµ¬í˜„.
- ëŒ€ê·œëª¨ MoE ìµœì í™”: `DeepSpeed`ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ë¶„ì‚° í•™ìŠµ.

ë”°ë¼ì„œ, íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ MoEë¥¼ ì ì ˆíˆ ì ìš©í•˜ë©´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸš€
